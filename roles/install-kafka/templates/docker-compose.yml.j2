version: '3'
services:
  zookeeper:
    image: '{{ image_repository }}/public/zookeeper-official:3.7'
    network_mode: host
    restart: always
    environment:
      ZOO_MY_ID: "{{ groups['kafka'].index(inventory_hostname) }}"
      ZOO_SERVERS: "{% for host in groups['kafka'] %}server.{{ groups['kafka'].index(host)|string +'='+ host +':2888:3888;2181'}} {% endfor %}"
    volumes:
      - "{{ kafka_root_dir }}/zookeeper/data:/data"
      - "{{ kafka_root_dir }}/zookeeper/datalog:/datalog"
      - "{{ kafka_root_dir }}/zookeeper/conf:/conf"
  kafka:
    image: '{{ image_repository }}/public/kafka:2.13-2.7.0'
    network_mode: host
    restart: always
    environment:
      KAFKA_ADVERTISED_PORT: 9092
      DOCKER_API_VERSION: 1.22
      KAFKA_BROKER_ID: "{{ groups['kafka'].index(inventory_hostname) }}"
      KAFKA_ADVERTISED_HOST_NAME: "{{ ansible_ssh_host }}"
      KAFKA_ZOOKEEPER_CONNECT: '{{ zk_hosts_str }}'
      KAFKA_HEAP_OPTS: '{{ kafka_heap_opts }}'
      #5个logstash消费者,每个8个线程，40个partition
      KAFKA_NUM_PARTITIONS: 40
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://{{ ansible_ssh_host }}:9092
      #cpu核数+1
      KAFKA_NUM_NETWORK_THREADS: 9
      #cpu的两倍
      KAFKA_NUM_IO_THREADS: 16
      #partition*2=log文件个数80， 80/3=28每台broker文件夹个数，每个segament默认1G，每个文件夹大小为(当前值+1)G,总大小不超过磁盘80%，2000G盘
      KAFKA_LOG_RETENTION_BYTES: 55000000000
      KAFKA_DEFAULT_REPLICATION_FACTOR: 2
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
      - "{{ kafka_root_dir }}/kafka:/kafka"
  kafka-exporter:
    image: '{{ image_repository }}/public/kafka-exporter:1.3.1'
    restart: always
    network_mode: host
    depends_on:
      - zookeeper
      - kafka
    command:
      - --log.enable-sarama
{% for host in groups['kafka'] %}
      - --kafka.server={{ host }}:9092
{% endfor %}



