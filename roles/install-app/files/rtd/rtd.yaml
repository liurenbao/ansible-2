NAME: rtdxxx
LAST DEPLOYED: Thu Oct 28 22:00:58 2021
NAMESPACE: rtd-test
STATUS: pending-install
REVISION: 1
TEST SUITE: None
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
imagePullSecrets:
- name: myregistrykey
ingress:
  enabled: false
nodeSelector: {}
podAnnotations: {}
podSecurityContext: {}
rtd_common:
  Host: rtd-test.fanli.inhuawei.com
  file_server_type: nas
  zk_enable: "true"
  zk_host_suffix: zookeeper
  zk_node: rtd
  zk_port: "2181"
ruleengine:
  PathPrefix: /ruleengine/
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - rtd-ruleengine
          topologyKey: zone
        weight: 100
  env:
  - name: SPRING_PROFILES_ACTIVE
    value: dev
  - name: SERVER_PORT
    value: "8999"
  - name: ZK_ENABLE
    value: "true"
  - name: ZK_HOST
    value: rtd-zookeeper
  - name: ZK_NODE
    value: rtd
  - name: FILE_SERVER_TYPE
    value: nas
  - name: IMPORT_FILE_PATH
    value: /data/ruleengine/resources
  - name: LOG_FILE_PATH
    value: /data/ruleengine/logs
  - name: DATA_FILE_PATH
    value: /data/ruleengine/data
  image:
    pullPolicy: Always
    repository: 10.243.36.7:80/rtd/ruleengine
    tag: 20211028.61338-develop
  replicaCount: 2
  resources:
    limits:
      cpu: "2"
      memory: 2Gi
    requests:
      cpu: 10m
      memory: 50Mi
  service:
    name: http
    port: 80
    protocol: TCP
    targetPort: 80
    type: ClusterIP
  volumeMounts:
  - mountPath: /data
    name: rtd-data
  volumes:
  - name: rtd-data
securityContext: {}
service:
  port: 80
  type: ClusterIP
serviceAccount:
  annotations: {}
  create: true
  name: ""
tolerations: []
web:
  PathPrefix: /
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - rtd-web
          topologyKey: zone
        weight: 100
  env:
  - name: SERVER_PORT
    value: "80"
  image:
    pullPolicy: Always
    repository: 10.243.36.7:80/rtd/rtd-web
    tag: 1.0.301
  replicaCount: 2
  resources:
    limits:
      cpu: "1"
      memory: 1024Mi
    requests:
      cpu: "0.1"
      memory: 100Mi
  service:
    name: http
    port: 80
    protocol: TCP
    targetPort: 80
    type: ClusterIP
workflow:
  PathPrefix: /workflow/
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - rtd-workflow
          topologyKey: zone
        weight: 100
  env:
    db_ip: 10.247.139.143
    db_port: "15400"
    db_user_name: RTD
    db_user_pwd: ENC(F8Rj+7/4BZwgsgrc8F9IxZrgclWSfKL6)
    ravencast_channel: /Rtd/Test/NextDispatchRequest
    ravencast_hosts: 10.247.4.8:10101;10.247.4.8:10201
    ravencast_server: rtd_server
  image:
    pullPolicy: Always
    repository: 10.243.36.7:80/rtd/rtd-workflow
    tag: 20211028.61060-dev
  replicaCount: 2
  resources:
    limits:
      cpu: "2"
      memory: 2Gi
    requests:
      cpu: 10m
      memory: 50Mi
  service:
    name: http
    port: 80
    protocol: TCP
    targetPort: 8080
    type: ClusterIP
zookeeper:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - zookeeper
          topologyKey: zone
        weight: 100
  allowAnonymousLogin: true
  auth:
    clientPassword: ""
    clientUser: ""
    enabled: false
    existingSecret: ""
    serverPasswords: ""
    serverUsers: ""
  autopurge:
    purgeInterval: 0
    snapRetainCount: 3
  clusterDomain: cluster.local
  common:
    exampleValue: common-chart
    global:
      imagePullSecrets: []
      imageRegistry: ""
      storageClass: ""
  commonAnnotations: {}
  commonLabels: {}
  config: ""
  containerPort: 2181
  customLivenessProbe: {}
  customReadinessProbe: {}
  dataLogDir: ""
  diagnosticMode:
    args:
    - infinity
    command:
    - sleep
    enabled: false
  electionContainerPort: 3888
  extraDeploy: []
  extraVolumeMounts: []
  extraVolumes: []
  followerContainerPort: 2888
  fourlwCommandsWhitelist: srvr, mntr, ruok
  fullnameOverride: ""
  global:
    imagePullSecrets: []
    imageRegistry: ""
    storageClass: ""
  heapSize: 1024
  hostAliases: []
  image:
    debug: false
    pullPolicy: IfNotPresent
    pullSecrets: []
    repository: 10.243.36.7:80/rtd/zookeeper
    tag: 3.7.0-debian-10-r127
  initContainers: []
  initLimit: 10
  jvmFlags: ""
  listenOnAllIPs: false
  livenessProbe:
    enabled: true
    failureThreshold: 6
    initialDelaySeconds: 30
    periodSeconds: 10
    probeCommandTimeout: 2
    successThreshold: 1
    timeoutSeconds: 5
  logLevel: ERROR
  maxClientCnxns: 60
  maxSessionTimeout: 40000
  metrics:
    containerPort: 9141
    enabled: false
    prometheusRule:
      enabled: false
      namespace: ""
      rules: []
      selector: {}
    service:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: '{{ .Values.metrics.service.port }}'
        prometheus.io/scrape: "true"
      port: 9141
      type: ClusterIP
    serviceMonitor:
      enabled: false
      interval: ""
      namespace: ""
      scrapeTimeout: ""
      selector: {}
  minServerId: 1
  nameOverride: ""
  namespaceOverride: ""
  networkPolicy:
    allowExternal: true
    enabled: false
  nodeAffinityPreset:
    key: ""
    type: ""
    values: []
  nodeSelector: {}
  persistence:
    accessModes:
    - ReadWriteMany
    annotations: {}
    dataLogDir:
      existingClaim: ""
      selector: {}
      size: 8Gi
    enabled: true
    existingClaim: ""
    selector: {}
    size: 8Gi
    storageClass: nfs-client
  podAffinityPreset: ""
  podAnnotations: {}
  podAntiAffinityPreset: soft
  podDisruptionBudget:
    maxUnavailable: 1
  podLabels: {}
  podManagementPolicy: Parallel
  preAllocSize: 65536
  priorityClassName: ""
  readinessProbe:
    enabled: true
    failureThreshold: 6
    initialDelaySeconds: 5
    periodSeconds: 10
    probeCommandTimeout: 2
    successThreshold: 1
    timeoutSeconds: 5
  replicaCount: 3
  resources:
    requests:
      cpu: 250m
      memory: 256Mi
  rollingUpdatePartition: ""
  schedulerName: ""
  securityContext:
    enabled: true
    fsGroup: 1001
    runAsUser: 1001
  service:
    annotations: {}
    disableBaseClientPort: false
    electionPort: 3888
    followerPort: 2888
    headless:
      annotations: {}
    loadBalancerIP: ""
    nodePorts:
      client: ""
      clientTls: ""
    port: 2181
    publishNotReadyAddresses: true
    tlsClientPort: 3181
    type: ClusterIP
  serviceAccount:
    automountServiceAccountToken: true
    create: false
    name: ""
  snapCount: 100000
  syncLimit: 5
  tickTime: 2000
  tls:
    client:
      autoGenerated: false
      enabled: false
      existingSecret: ""
      keystorePassword: ""
      keystorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.keystore.jks
      passwordsSecretName: ""
      truststorePassword: ""
      truststorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.truststore.jks
    quorum:
      autoGenerated: false
      enabled: false
      existingSecret: ""
      keystorePassword: ""
      keystorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.keystore.jks
      passwordsSecretName: ""
      truststorePassword: ""
      truststorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.truststore.jks
    resources:
      limits: {}
      requests: {}
  tlsContainerPort: 3181
  tolerations: []
  topologySpreadConstraints: {}
  updateStrategy: RollingUpdate
  volumePermissions:
    enabled: false
    image:
      pullPolicy: Always
      pullSecrets: []
      repository: bitnami/bitnami-shell
      tag: 10-debian-10-r172
    resources: {}

HOOKS:
MANIFEST:
---
# Source: rtd/charts/zookeeper/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: rtdxxx-zookeeper
  namespace: rtd-test
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-7.4.3
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: rtdxxx
      app.kubernetes.io/component: zookeeper
  maxUnavailable: 1
---
# Source: rtd/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rtdxxx
  labels:
    helm.sh/chart: rtd-0.1.0
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: rtd/charts/zookeeper/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: rtdxxx-zookeeper-headless
  namespace: rtd-test
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-7.4.3
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    
    - name: tcp-client
      port: 2181
      targetPort: client
    
    
    - name: follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/component: zookeeper
---
# Source: rtd/charts/zookeeper/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: rtdxxx-zookeeper
  namespace: rtd-test
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-7.4.3
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  ports:
    
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    
    
    - name: follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/component: zookeeper
---
# Source: rtd/templates/service-ruleengine.yaml
apiVersion: v1
kind: Service
metadata:
  name: rtdxxx-ruleengine
  labels:
    helm.sh/chart: rtd-0.1.0
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: rtd-ruleengine
    app.kubernetes.io/component: rtd-ruleengine
    helm.sh/chart: rtd-0.1.0
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: rtd/templates/service-web.yaml
apiVersion: v1
kind: Service
metadata:
  name: rtdxxx-web
  labels:
    helm.sh/chart: rtd-0.1.0
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: rtd-web
    app.kubernetes.io/component: rtd-web
    helm.sh/chart: rtd-0.1.0
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: rtd/templates/service-workflow.yaml
apiVersion: v1
kind: Service
metadata:
  name: rtdxxx-workflow
  labels:
    helm.sh/chart: rtd-0.1.0
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8080
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: rtd-workflow
    app.kubernetes.io/component: rtd-workflow
    helm.sh/chart: rtd-0.1.0
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: rtd/templates/deployment-ruleengine.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rtdxxx-ruleengine
  labels:
    app.kubernetes.io/name: rtd-ruleengine
    app.kubernetes.io/component: rtd-ruleengine
    helm.sh/chart: rtd-0.1.0
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: rtd-ruleengine
      app.kubernetes.io/component: rtd-ruleengine
      helm.sh/chart: rtd-0.1.0
      app.kubernetes.io/instance: rtdxxx
      app.kubernetes.io/version: "1.16.0"
      app.kubernetes.io/managed-by: Helm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: rtd-ruleengine
        app.kubernetes.io/component: rtd-ruleengine
        helm.sh/chart: rtd-0.1.0
        app.kubernetes.io/instance: rtdxxx
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      imagePullSecrets:
        - name: myregistrykey
      serviceAccountName: rtdxxx
      securityContext:
        {}
      containers:
        - name: rtdxxx-ruleengine
          securityContext:
            {}
          image: "10.243.36.7:80/rtd/ruleengine:20211028.61338-develop"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          env:
            - name: SPRING_PROFILES_ACTIVE
              value: dev
            - name: SERVER_PORT
              value: '8999'
            - name: ZK_ENABLE
              value: "true"
            - name: ZK_HOST
              value: rtdxxx_"zookeeper"
            - name: ZK_NODE
              value: "rtd"
            - name: FILE_SERVER_TYPE
              value: "nas"
            - name: IMPORT_FILE_PATH
              value: '/data/ruleengine/resources'
            - name: LOG_FILE_PATH
              value: '/data/ruleengine/logs'
            - name: DATA_FILE_PATH
              value: '/data/ruleengine/data'

          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 10m
              memory: 50Mi
          volumeMounts:
            - mountPath: /data
              name: rtd-data
      volumes:
            - name: rtd-data
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - rtd-ruleengine
              topologyKey: zone
            weight: 100
---
# Source: rtd/templates/deployment-web.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rtdxxx-web
  labels:
    app.kubernetes.io/name: rtd-web
    app.kubernetes.io/component: rtd-web
    helm.sh/chart: rtd-0.1.0
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: rtd-web
      app.kubernetes.io/component: rtd-web
      helm.sh/chart: rtd-0.1.0
      app.kubernetes.io/instance: rtdxxx
      app.kubernetes.io/version: "1.16.0"
      app.kubernetes.io/managed-by: Helm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: rtd-web
        app.kubernetes.io/component: rtd-web
        helm.sh/chart: rtd-0.1.0
        app.kubernetes.io/instance: rtdxxx
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      imagePullSecrets:
        - name: myregistrykey
      serviceAccountName: rtdxxx
      securityContext:
        {}
      containers:
        - name: rtdxxx-web
          securityContext:
            {}
          image: "10.243.36.7:80/rtd/rtd-web:1.0.301"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: 80
            initialDelaySeconds: 200
            timeoutSeconds: 10
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /
              port: 80
            timeoutSeconds: 10
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3

          resources:
            limits:
              cpu: "1"
              memory: 1024Mi
            requests:
              cpu: "0.1"
              memory: 100Mi
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - rtd-web
              topologyKey: zone
            weight: 100
---
# Source: rtd/templates/deployment-workflow.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rtdxxx-workflow
  labels:
    app.kubernetes.io/name: rtd-workflow
    app.kubernetes.io/component: rtd-workflow
    helm.sh/chart: rtd-0.1.0
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: rtd-workflow
      app.kubernetes.io/component: rtd-workflow
      helm.sh/chart: rtd-0.1.0
      app.kubernetes.io/instance: rtdxxx
      app.kubernetes.io/version: "1.16.0"
      app.kubernetes.io/managed-by: Helm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: rtd-workflow
        app.kubernetes.io/component: rtd-workflow
        helm.sh/chart: rtd-0.1.0
        app.kubernetes.io/instance: rtdxxx
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      imagePullSecrets:
        - name: myregistrykey
      serviceAccountName: rtdxxx
      securityContext:
        {}
      containers:
        - name: rtdxxx-workflow
          securityContext:
            {}
          image: "10.243.36.7:80/rtd/rtd-workflow:20211028.61060-dev"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          env:
            - name: SPRING_PROFILES_ACTIVE
              value: test
            - name: SERVER_PORT
              value: '8080'
            - name: DB_IP
              value: "10.247.139.143"
            - name: DB_PORT
              value: "15400"
            - name: DB_USER_NAME
              value: "RTD"
            - name: DB_USER_PWD
              value: "ENC(F8Rj+7/4BZwgsgrc8F9IxZrgclWSfKL6)"
            - name: DB_USER_NAME_QUARTZ
              value: RTD_QUARZ
            - name: WORKFLOW_ENGINE_ENDPOINT
              value: rtdxxx-ruleengine.rtd-test:80
            - name: DATASOURCE_AGENT_ENDPOINT
              value: rtd-datasource.rtd:80
            - name: FILE_SERVER_TYPE
              value: NAS
            - name: FILE_PATH
              value: /usr/importFile
            - name: ZK_TIME_OUT
              value: '40000'
            - name: ZK_HOST
              value: rtdxxx_
            - name: ZK_PORT
              value: "2181"
            - name: RTD_NODE_PATH
              value: "rtd"
            - name: GRPC_ENDPOINT
              value: rtd-ruleengine
            - name: GRPC_ENDPOINT_PORT
              value: '8999'
            - name: ZOOKEEPER_ENABLE
              value: "true"
            - name: RAVENCAST_ENABLE
              value: 'true'
            - name: RAVENCAST_HOSTS
              value: "10.247.4.8:10101;10.247.4.8:10201"
            - name: RAVENCAST_SESSION
              value: 
            - name: RAVENCAST_CHANNEL
              value: "/Rtd/Test/NextDispatchRequest"
        
          livenessProbe:
            httpGet:
              path: /workflow/v1/health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 400
            timeoutSeconds: 10
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
        
          readinessProbe:
            httpGet:
              path: /workflow/v1/health
              port: 8080
              scheme: HTTP
            timeoutSeconds: 10
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3

          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 10m
              memory: 50Mi
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - rtd-workflow
              topologyKey: zone
            weight: 100
---
# Source: rtd/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: rtdxxx-zookeeper
  namespace: rtd-test
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-7.4.3
    app.kubernetes.io/instance: rtdxxx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  serviceName: rtdxxx-zookeeper-headless
  replicas: 3
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: rtdxxx
      app.kubernetes.io/component: zookeeper
  template:
    metadata:
      name: rtdxxx-zookeeper
      labels:
        app.kubernetes.io/name: zookeeper
        helm.sh/chart: zookeeper-7.4.3
        app.kubernetes.io/instance: rtdxxx
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: zookeeper
    spec:
      
      serviceAccountName: default
      securityContext:
        fsGroup: 1001
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - zookeeper
              topologyKey: zone
            weight: 100
      containers:
        - name: zookeeper
          image: 10.243.36.7:80/rtd/zookeeper:3.7.0-debian-10-r127
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - bash
            - -ec
            - |
                # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
                # check ZOO_SERVER_ID in persistent volume via myid
                # if not present, set based on POD hostname
                if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
                  export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
                else
                  HOSTNAME=`hostname -s`
                  if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
                    ORD=${BASH_REMATCH[2]}
                    export ZOO_SERVER_ID=$((ORD + 1 ))
                  else
                    echo "Failed to get index from hostname $HOST"
                    exit 1
                  fi
                fi
                exec /entrypoint.sh /run.sh
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_PRE_ALLOC_SIZE
              value: "65536"
            - name: ZOO_SNAPCOUNT
              value: "100000"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: "srvr, mntr, ruok"
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "0"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "3"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: rtdxxx-zookeeper-0.rtdxxx-zookeeper-headless.rtd-test.svc.cluster.local:2888:3888::1 rtdxxx-zookeeper-1.rtdxxx-zookeeper-headless.rtd-test.svc.cluster.local:2888:3888::2 rtdxxx-zookeeper-2.rtdxxx-zookeeper-headless.rtd-test.svc.cluster.local:2888:3888::3 
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "1024"
            - name: ZOO_LOG_LEVEL
              value: "ERROR"
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
  volumeClaimTemplates:
    - metadata:
        name: data
        annotations:
      spec:
        accessModes:
          - "ReadWriteMany"
        resources:
          requests:
            storage: "8Gi"
        storageClassName: nfs-client
---
# Source: rtd/templates/ingressRouter-ruleengine.yaml
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  annotations:
  generation: 1
  name: rtdxxx-ruleengine
spec:
  entryPoints:
    - web
  routes:
    - kind: Rule
      match: Host(`rtd-test.fanli.inhuawei.com`)&&PathPrefix(`/ruleengine/`)
      services:
        - name: rtdxxx-ruleengine
          port: 80
---
# Source: rtd/templates/ingressRouter-web.yaml
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  annotations:
  generation: 1
  name: rtdxxx-web
spec:
  entryPoints:
    - web
  routes:
    - kind: Rule
      match: Host(`rtd-test.fanli.inhuawei.com`)&&PathPrefix(`/`)
      services:
        - name: rtdxxx-web
          port: 80
---
# Source: rtd/templates/ingressRouter-workflow.yaml
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  annotations:
  generation: 1
  name: rtdxxx-workflow
spec:
  entryPoints:
    - web
  routes:
    - kind: Rule
      match: Host(`rtd-test.fanli.inhuawei.com`)&&PathPrefix(`/workflow/`)
      services:
        - name: rtdxxx-workflow
          port: 80

NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace rtd-test -l "app.kubernetes.io/name=rtd,app.kubernetes.io/instance=rtdxxx" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace rtd-test $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace rtd-test port-forward $POD_NAME 8080:$CONTAINER_PORT
