# The job name assigned to scraped metrics by default.
job_name: <job_name>

  # How frequently to scrape targets from this job.
  [ scrape_interval: <duration> | default = <global_config.scrape_interval> ]

  # Per-scrape timeout when scraping this job.
  [ scrape_timeout: <duration> | default = <global_config.scrape_timeout> ]

  # The HTTP resource path on which to fetch metrics from targets.
  [ metrics_path: <path> | default = /metrics ]

  # honor_labels controls how Prometheus handles conflicts between labels that are
  # already present in scraped data and labels that Prometheus would attach
  # server-side ("job" and "instance" labels, manually configured target
  # labels, and labels generated by service discovery implementations).
  #
  # If honor_labels is set to "true", label conflicts are resolved by keeping label
  # values from the scraped data and ignoring the conflicting server-side labels.
  #
  # If honor_labels is set to "false", label conflicts are resolved by renaming
  # conflicting labels in the scraped data to "exported_<original-label>" (for
  # example "exported_instance", "exported_job") and then attaching server-side
  # labels.
  #
  # Setting honor_labels to "true" is useful for use cases such as federation and
  # scraping the Pushgateway, where all labels specified in the target should be
  # preserved.
  #
  # Note that any globally configured "external_labels" are unaffected by this
  # setting. In communication with external systems, they are always applied only
  # when a time series does not have a given label yet and are ignored otherwise.
  [ honor_labels: <boolean> | default = false ]

  # honor_timestamps controls whether Prometheus respects the timestamps present
  # in scraped data.
  #
  # If honor_timestamps is set to "true", the timestamps of the metrics exposed
  # by the target will be used.
  #
  # If honor_timestamps is set to "false", the timestamps of the metrics exposed
  # by the target will be ignored.
  [ honor_timestamps: <boolean> | default = true ]

  # Configures the protocol scheme used for requests.
  [ scheme: <scheme> | default = http ]

# Optional HTTP URL parameters.
params:
  [ <string>: [ <string>, ... ] ]

# Sets the `Authorization` header on every scrape request with the
# configured username and password.
# password and password_file are mutually exclusive.
basic_auth:
  [ username: <string> ]
    [ password: <secret> ]
    [ password_file: <string> ]

# Sets the `Authorization` header on every scrape request with
# the configured credentials.
authorization:
  # Sets the authentication type of the request.
    [ type:
        <string> | default: Bearer ]
    # Sets the credentials of the request. It is mutually exclusive with
    # `credentials_file`.
    [ credentials: <secret> ]
    # Sets the credentials of the request with the credentials read from the
    # configured file. It is mutually exclusive with `credentials`.
    [ credentials_file: <filename> ]

# Optional OAuth 2.0 configuration.
# Cannot be used at the same time as basic_auth or authorization.
oauth2:
  [ <oauth2> ]

    # Configure whether scrape requests follow HTTP 3xx redirects.
    [ follow_redirects: <bool> | default = true ]

# Configures the scrape request's TLS settings.
tls_config:
  [ <tls_config> ]

    # Optional proxy URL.
    [ proxy_url: <string> ]

# List of Azure service discovery configurations.
azure_sd_configs:
  [ - <azure_sd_config> ... ]

# List of Consul service discovery configurations.
consul_sd_configs:
  [ - <consul_sd_config> ... ]

# List of DigitalOcean service discovery configurations.
digitalocean_sd_configs:
  [ - <digitalocean_sd_config> ... ]

# List of Docker service discovery configurations.
docker_sd_configs:
  [ - <docker_sd_config> ... ]

# List of Docker Swarm service discovery configurations.
dockerswarm_sd_configs:
  [ - <dockerswarm_sd_config> ... ]

# List of DNS service discovery configurations.
dns_sd_configs:
  [ - <dns_sd_config> ... ]

# List of EC2 service discovery configurations.
ec2_sd_configs:
  [ - <ec2_sd_config> ... ]

# List of Eureka service discovery configurations.
eureka_sd_configs:
  [ - <eureka_sd_config> ... ]

# List of file service discovery configurations.
file_sd_configs:
  [ - <file_sd_config> ... ]

# List of GCE service discovery configurations.
gce_sd_configs:
  [ - <gce_sd_config> ... ]

# List of Hetzner service discovery configurations.
hetzner_sd_configs:
  [ - <hetzner_sd_config> ... ]

# List of HTTP service discovery configurations.
http_sd_configs:
  [ - <http_sd_config> ... ]

# List of Kubernetes service discovery configurations.
kubernetes_sd_configs:
  [ - <kubernetes_sd_config> ... ]

# List of Lightsail service discovery configurations.
lightsail_sd_configs:
  [ - <lightsail_sd_config> ... ]

# List of Linode service discovery configurations.
linode_sd_configs:
  [ - <linode_sd_config> ... ]

# List of Marathon service discovery configurations.
marathon_sd_configs:
  [ - <marathon_sd_config> ... ]

# List of AirBnB's Nerve service discovery configurations.
nerve_sd_configs:
  [ - <nerve_sd_config> ... ]

# List of OpenStack service discovery configurations.
openstack_sd_configs:
  [ - <openstack_sd_config> ... ]

# List of Scaleway service discovery configurations.
scaleway_sd_configs:
  [ - <scaleway_sd_config> ... ]

# List of Zookeeper Serverset service discovery configurations.
serverset_sd_configs:
  [ - <serverset_sd_config> ... ]

# List of Triton service discovery configurations.
triton_sd_configs:
  [ - <triton_sd_config> ... ]

# List of labeled statically configured targets for this job.
static_configs:
  [ - <static_config> ... ]

# List of target relabel configurations.
relabel_configs:
  [ - <relabel_config> ... ]

# List of metric relabel configurations.
metric_relabel_configs:
  [ - <relabel_config> ... ]

    # An uncompressed response body larger than this many bytes will cause the
    # scrape to fail. 0 means no limit. Example: 100MB.
    # This is an experimental feature, this behaviour could
    # change or be removed in the future.
    [ body_size_limit: <size> | default = 0 ]
    # Per-scrape limit on number of scraped samples that will be accepted.
    # If more than this number of samples are present after metric relabeling
    # the entire scrape will be treated as failed. 0 means no limit.
    [ sample_limit: <int> | default = 0 ]

    # Per-scrape limit on number of labels that will be accepted for a sample. If
    # more than this number of labels are present post metric-relabeling, the
    # entire scrape will be treated as failed. 0 means no limit.
    [ label_limit: <int> | default = 0 ]

    # Per-scrape limit on length of labels name that will be accepted for a sample.
    # If a label name is longer than this number post metric-relabeling, the entire
    # scrape will be treated as failed. 0 means no limit.
    [ label_name_length_limit: <int> | default = 0 ]

    # Per-scrape limit on length of labels value that will be accepted for a sample.
    # If a label value is longer than this number post metric-relabeling, the
    # entire scrape will be treated as failed. 0 means no limit.
    [ label_value_length_limit: <int> | default = 0 ]

    # Per-scrape config limit on number of unique targets that will be
    # accepted. If more than this number of targets are present after target
    # relabeling, Prometheus will mark the targets as failed without scraping them.
    # 0 means no limit. This is an experimental feature, this behaviour could
    # change in the future.
    [ target_limit: <int> | default = 0 ]